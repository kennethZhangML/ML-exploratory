{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Standardization\n",
    "z = (x - u) / q\n",
    "\n",
    "For each value, x, we subtract the overall mean of the data, \n",
    "then divide by the overall standard deviation q.\n",
    "\n",
    "The new value, z, represents the standardized data value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "pizza_data = data[data['food_type'] == 'pizza']\n",
    "\n",
    "col_standardized = scale(pizza_data)\n",
    "\n",
    "col_means = col_standardized.mean(axis = 0).round(decimals = 3)\n",
    "\n",
    "col_std = col_standardized.std(axis = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Range Scaling \n",
    "\n",
    "Scale data by compressing it into a fixed range. \n",
    "Allows better view of the data in terms of proportions or percentages, based on the minimum and maximum values in the data.\n",
    "\n",
    "x_prop = (x - d_min) / (d_max - d_min)\n",
    "\n",
    "- for a given value x, we compute the proportion of the value with respect to the minimum and maximum of the data (d_min, d_max).\n",
    "- formula given above computes the proportion of the data value, x_prop\n",
    "- use the proportion of the value to scale to the specified range [r_min, r_max]\n",
    "The formula below calculates the new scaled value x_scale\n",
    "\n",
    "x_scale = x_prop * (r_max - r_min) + r_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "default_scaler = MinMaxScaler()\n",
    "transformed = default_scaler.fit_transform(data)\n",
    "\n",
    "custom_scaler = MinMaxScaler(feature_range = (-2, 3)) #compress data within a given range\n",
    "transformed = custom_scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "new_data = data\n",
    "\n",
    "default_scaler = MinMaxScaler()\n",
    "transformed = default_scaler.fit_transform(new_data)\n",
    "\n",
    "default_Scaler = MinMaxScaler()\n",
    "default_scaler.fit(data)\n",
    "transformed = default_scaler.transform(new_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Scaling\n",
    "\n",
    "Data standardization uses each feature's mean and standard deviation while ranged scaling uses the maximum and minimum feature vakues meaning they're both susceptible to being skewed by outlier values.\n",
    "\n",
    "Robustly scale the data by using data's median and IQR.\n",
    "Since median and IQR are percentile measurements of the data, they are not affected by outliers -> subtract median from each data then scale to IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "robust_scaler = RobustScaler()\n",
    "transformed = robust_scaler.fit_transform(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Normalization\n",
    "\n",
    "- When we wish to scale individual data observations\n",
    "e.g., when we want to cluster data -> calculating cosine similarity scores\n",
    "\n",
    "L2 normalization applied to a particular row of data array will:\n",
    "- divide each value in that row by the row's L2 norm\n",
    "- L2 Norm: square root of the sum of squared values for the row\n",
    "\n",
    "![](L2Norm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer = Normalizer() # Implements the L2 Normalizations\n",
    "transformed = normalizer.fit_transform(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data imputation Methods\n",
    "\n",
    "- some datasets contain missing values\n",
    "- perform data imputation on little missing values\n",
    "\n",
    "4 Methods of Imputing:\n",
    "- mean values\n",
    "- median value\n",
    "- frequent value\n",
    "- missing values with a constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp_mean = SimpleImputer(strategy = 'mean')\n",
    "transformed = imp_mean.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_median = SimpleImputer(strategy = 'median')\n",
    "transformed = imp_median.fit_transform(data)\n",
    "\n",
    "imp_frequent = SimpleImputer(strategy = 'most_frequent')\n",
    "transformed = imp_frequent.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_constant = SimpleImputer(strategy='constant',\n",
    "                             fill_value=-1)\n",
    "transformed = imp_constant.fit_transform(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "- data contains correlated numeric features, perform principal component analysis for dimensionality reduction\n",
    "- PCA extracts principal components which are uncorrelated sets of latent variables that encompass most of the information from the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "\n",
    "pca_obj = PCA()\n",
    "pc = pca_obj.fit_transform(data).round(3)\n",
    "\n",
    "pca_obj2 = PCA(n_components = 3)\n",
    "pc = pca_obj.fit_transform(data).round(3)\n",
    "\n",
    "def pca_data(data, n_components):\n",
    "  pca_obj = PCA(n_components = n_components)\n",
    "  component_data = pca_obj.fit_transform(data)\n",
    "  return component_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n",
      "(569,)\n",
      "['malignant' 'benign']\n",
      "(212, 30)\n",
      "(357, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "bc = load_breast_cancer()\n",
    "print(bc.data.shape)\n",
    "print(bc.target)\n",
    "print(bc.target.shape)\n",
    "print(bc.target_names)\n",
    "\n",
    "malignant = bc.data[bc.target == 0]\n",
    "print(malignant.shape)\n",
    "\n",
    "benign = bc.data[bc.target == 1]\n",
    "print(benign.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
